---
title: "STAT403HW2"
author: "Alice Roberts"
date: "3/17/2019"
output: html_document
---

## Entire R code for project is  below: 

```{r }
#install.packages("FactoMineR")
library(FactoMineR)
#install.packages("mice")
library(mice)
#install.packages("VIM")
library(VIM)
#install.packages("stringr")
library(stringr)
#install.packages("dplyr")
library(dplyr)
#install.packages("MASS")
library(MASS)
#install.packages("lme4")
library(lme4)
#install.packages("glmnet")
library(glmnet)
library(car)
```

We start by loading in the data file and finding the number of NAs in the dataset.

```{r }
df <- read.csv("~/Desktop/Stat_403_650_890_-_Analysis_Assignment_2_-_Due_Mar_19/CCHS Data for Stat 403 650 890.csv", comment.char="#")
head(df)

#finding the total number of NAs in the Dataframe
numNA<-sum(is.na(df))
numNA
##finding the total number of rows with NA 
10000-length(which(rowSums(is.na(df))==0))
```
Based on the result above, we can conclude that our response variable (CCCA_121) is binary; yes or no. 
Now, we change the column names to make it easier to understand and change all categorical variables to factors so that we can use logistic regression because our response variable is binary. 

```{r}
## Changing the column names of the dataframe to make it easier to understand 

colnames(df) <- c("HeartDisease", "arthritis_or_rheumatism", "Age","Sex", "Marital_Status", "racialorigin", "Immigrantstatus", "education", "Total_household_income", "BMI","Physical_activity_index", "regular_medical_doctor", "smoker", "drinker", "high_blood_pressure", "diabetes", "emphysema_chronic_obstructive_pulmonary_disease", "total_fruits_vegetables","Self_perceived_stress", "Province", "Sampling_weight")

head(df)

#changing all of our categorical variables as factors

df$HeartDisease<-as.factor(df$HeartDisease)
df$arthritis_or_rheumatism<-as.factor(df$arthritis_or_rheumatism)
df$Sex<-as.factor(df$Sex)
df$Marital_Status <- as.factor(df$Marital_Status)
df$racialorigin <- as.factor(df$racialorigin)
df$Immigrantstatus <- as.factor(df$Immigrantstatus)
df$education <- as.factor(df$education)
df$Physical_activity_index <- as.factor(df$Physical_activity_index)
df$regular_medical_doctor <- as.factor(df$regular_medical_doctor)
df$smoker <- as.factor(df$smoker)
df$drinker <- as.factor(df$drinker)
df$high_blood_pressure <- as.factor(df$high_blood_pressure)
df$diabetes <- as.factor(df$diabetes)
df$emphysema_chronic_obstructive_pulmonary_disease <- as.factor(df$emphysema_chronic_obstructive_pulmonary_disease)
df$Self_perceived_stress <- as.factor(df$Self_perceived_stress)
df$Province <- as.factor(df$Province)

# Looking at the summary for the dataframe
summary(df)


```


```{r}
#before we implement Mice for the NAs we will now reorder the dataframe so that all 
#categorical variables are beside each other, this will help with data manipulation 
#tutorial can be found here : https://datascienceplus.com/imputing-missing-data-with-r-mice-package/

df<- df[c(1,2,4,5,6,7,8,11,12,13,14,15,16,17,19,20,3,9,10,18,21 )]

## now all of our categroical variabels should be from rows 1:16
head(df) 



#First, we can explore the pattern of missing data
aggr_plot <- aggr(df, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(data), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"), main="plot of NA pattern")

#we are using predictive mean matching as the imputation method for MICE
tempData <- mice(df,m=5,maxit=5,meth='pmm',seed=500)
summary(tempData)

completedf <- complete(tempData,1)
newdf<-completedf
head(newdf)

data.frame(unique(newdf$Total_household_income))


```


Now we wish to deal with Age and Income, as stated in class we will choose the the middle number for the range in age and income. 
```{r}
newdf$Age <- newdf$Age %>%
  str_replace_all("30 TO 34 YEARS", "32") %>%
  str_replace_all("35 TO 39 YEARS", "37") %>%
  str_replace_all("40 TO 44 YEARS", "42") %>%
  str_replace_all("45 TO 49 YEARS", "47") %>%
  str_replace_all("50 TO 54 YEARS", "52") %>%
  str_replace_all("55 TO 59 YEARS", "57") %>%
  str_replace_all("60 TO 64 YEARS", "62") 



# clean Household Income data
  # fill NAs with values from above
newdf <- newdf %>%
 mutate(Total_household_income = case_when(Total_household_income == "$15,000-$29,999" ~ 22000,
                                     Total_household_income == "$30,000-$49,999" ~ 40000,
                                     Total_household_income == "$50,000-$79,999" ~ 65000,
                                     Total_household_income == "$80,000 OR MORE" ~ 90000,
                                     Total_household_income == "LESS THAN 15,000" ~ 10000,
                                     Total_household_income == "NO INCOME" ~ 0))


```

Now we wish to apply logistic regression to our Repsonse Variable 
Note that for the full model we do not use the weights argument for the glm because our repsonse variable is a binary response variable. 

```{r}

newdf$Sampling_weight <- NULL 
head(newdf)


empty.mod <- glm(formula = HeartDisease ~ 1, family = binomial(link = logit), data = newdf)
full.mod <- glm(formula = HeartDisease ~ ., family = binomial(link = logit), data = newdf)

## Forward Selection 


forw.sel <- step(object = empty.mod, scope = list(upper = full.mod), direction = "forward", k = log(nrow(newdf)), trace = TRUE)

anova(forw.sel)


##Backward Selection 
 
back.sel <- step(object = full.mod, scope = list(lower = empty.mod), direction = "backward", k = log(nrow(newdf)), trace = TRUE)

anova(back.sel)


```



```{r}


## LASSO 
yy <- as.matrix(newdf[,1])
xx <- as.matrix(newdf[,2:20])


x_train <- model.matrix( ~ .-1, newdf[,2:20])


lm = cv.glmnet(x=x_train,y = as.factor(newdf$HeartDisease), intercept=FALSE ,family =   "binomial", alpha=1, nfolds=7)
best_lambda <- lm$lambda[which.min(lm$cvm)]

# Default plot method for cv.lambda() produces CV errors +/- 1 SE at each lambda.
plot(lm)
# Print out coefficients at optimal lambda
coef(lm)

##finding the probabilities associated with each row(person)
predict.las <- predict(lm, newx = x_train, type ="response")

plot(predict.las, type="p", main = "Plot of Predictions for heart disease per person",xlab="row/person", ylab="probability of heart disease")


##calculating the prediciton accuracy
mean(rep(0, length(newdf$HeartDisease)) == ifelse(newdf$HeartDisease == "NO", 0, 1))


```






In this homework assignment we wish to predict the cardiovascular disease (CCCA_121) based on the data set provided. 


## 1) Describe the response variable. Is it continuous / categorical / binary? What does its data type imply about the modeling method that is likely to work best?

CCAA_121 (Heart Disease) is our response variable and it is binary (yes or no).

We should use a logistic regression model because we have a binary response variable.

## 2) Describe the other variables. Which are continuous? Fixed effects? Random effects? Are there any variables that are NOT explanatory or a response? (e.g. IDs, weights)
Most of the other variables provided are categorical variables, besides age, total house income, BMI, total fruit and vegetable consumption and sampling weights. Age and total house income are discrete variables, and the other numerical variables are continous. 

We know immediately that the sampling wieghts are not an explanatory variable so we exclude it from the variable selection LASSO. 

Based on our variable selection technique LASSO, we see that we only need to include arthritis, sex, marital status, eduction, smoker, drinker, highblood presure, diabetes, emphysemapulmonarydisease, province, age, income and BMI in our logistic regression model. 

Thus the other variables not listed above will be the non explanatory variables.

## 3) Describe the missingness pattern.	Are there any missing values? Which variables? Are the missing values randomly assigned, or are is there some observable pattern? What are you going to do about the missing data?
```{r}
#finding the total number of NAs in the Dataframe
numNA<-sum(is.na(df))
numNA
10000-length(which(rowSums(is.na(df))==0))
#First, we can explore the pattern of missing data
aggr_plot <- aggr(df, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(data), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"), main="plot of NA pattern")
```
	
Yes, there is a lot of missing data about 13,918 cells of missing data and about 5451 rows of data that have NA terms. Since there was so many NAs I decided to implement MICE in order to fill in the NAs based on a weighted algorithm. 

Based on the plot I produced in R, we can see that age and BMI are the variables that have the most NAs. We can infer from the plot and the data that certain NAs are clustered and this may be because of the fields that were required based on the Province. Seeing that the responses were sampled across Provinces, there may have been different required fields based on the Province. 

## 4) How are you going to decide which variables to use. Note that only the prediction accuracy matters (lowest deviance/AIC/BIC)?	
Since we only care about the prediction accuracy, I decided to use LASSO as my variable selection technique. This is because LASSO is used primarily as a variable selection tool or for making predictions where interval estimates are not required. 
 
## 5) Describe your final model. How well does it fit the response?	Are there any possible colliinearity / VIF issues? How much better/worse is the model when the arthritis variable CCCA_051 is in it?
From LASSO we have the following output: 


```{r}


## LASSO 
yy <- as.matrix(newdf[,1])
xx <- as.matrix(newdf[,2:20])


x_train <- model.matrix( ~ .-1, newdf[,2:20])


lm = cv.glmnet(x=x_train,y = as.factor(newdf$HeartDisease), intercept=FALSE ,family =   "binomial", alpha=1, nfolds=7)
best_lambda <- lm$lambda[which.min(lm$cvm)]

# Default plot method for cv.lambda() produces CV errors +/- 1 SE at each lambda.
plot(lm)


##finding the probabilities associated with each row(person)
predict.las <- predict(lm, newx = x_train, type ="response")

#printing out the first 15 probabilties of heart disease 
head(round(predict.las,digits= 3),n=15)


#plotting the probabilities for heart disease per person
plot(predict.las, type="p", main = "Plot of Predictions for heart disease per person",xlab="row/person", ylab="probability of heart disease")


##calculating the prediciton accuracy of model probabilities
accuracy <- mean(rep(0, length(newdf$HeartDisease)) == ifelse(newdf$HeartDisease == "NO", 0, 1))
accuracy 

```

I have presented the probabilites of heart disease per row(per person) above. My model has a prediction accuracy of about 94.08%. 
Below we will show the full LASSO model with and without the arthritis variable. 
$H_0$: coefficient for the arthritis variable is 0. 
$H_a$: coefficient for the arthritis variable not 0. 

```{r}

Ho<-glm(formula=newdf$HeartDisease ~ 1, family=binomial(link=logit), data=newdf )
Ha<-glm(formula=newdf$HeartDisease ~ arthritis_or_rheumatism, family=binomial(link=logit), data=newdf )

anova(Ho,Ha, test ="Chisq")

```

We see based on the output produced above that the p-value is significant, thus we reject the null hypotheisis and conclude that the arthiritis variable is an explanatory variable for heart disease thus the model would be a better fit if we include this variable.


